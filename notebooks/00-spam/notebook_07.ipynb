{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAM or HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mantunes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/mantunes/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mantunes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mantunes/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/mantunes/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import nltk\n",
    "import tqdm\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "plt.rcParams['figure.dpi'] = 72\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, f1_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nltk_pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "\n",
    "def _nltk_pos_lemmatizer(lemmatizer, token, tag):\n",
    "    if tag is None:\n",
    "        return lemmatizer.lemmatize(token)\n",
    "    else:        \n",
    "        return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "\n",
    "def text_pre_processing(txt, m=2):\n",
    "    if txt is not None:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "        tokens = nltk.word_tokenize(txt)\n",
    "        tokens = [w for w in tokens if w.isalpha()]\n",
    "        tokens = nltk.pos_tag(tokens)\n",
    "        tokens = [(t[0], _nltk_pos_tagger(t[1])) for t in tokens]\n",
    "        tokens = [_nltk_pos_lemmatizer(lemmatizer, w, t).lower() for w,t in tokens]\n",
    "        tokens = [w for w in tokens if len(w) > m]\n",
    "        tokens = [w for w in tokens if w not in stop_words]\n",
    "    else:\n",
    "        tokens = []\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5_521, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Target</th><th>SMS</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;ham&quot;</td><td>&quot;Go until jurong point, crazy..…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Ok lar... Joking wif u oni...&quot;</td></tr><tr><td>&quot;spam&quot;</td><td>&quot;Free entry in 2 a wkly comp to…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;U dun say so early hor... U c …</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Nah I don&#x27;t think he goes to u…</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;spam&quot;</td><td>&quot;This is the 2nd time we have t…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Will �_ b going to esplanade f…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Pity, * was in mood for that. …</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;The guy did some bitching but …</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Rofl. Its true to its name&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5_521, 2)\n",
       "┌────────┬─────────────────────────────────┐\n",
       "│ Target ┆ SMS                             │\n",
       "│ ---    ┆ ---                             │\n",
       "│ str    ┆ str                             │\n",
       "╞════════╪═════════════════════════════════╡\n",
       "│ ham    ┆ Go until jurong point, crazy..… │\n",
       "│ ham    ┆ Ok lar... Joking wif u oni...   │\n",
       "│ spam   ┆ Free entry in 2 a wkly comp to… │\n",
       "│ ham    ┆ U dun say so early hor... U c … │\n",
       "│ ham    ┆ Nah I don't think he goes to u… │\n",
       "│ …      ┆ …                               │\n",
       "│ spam   ┆ This is the 2nd time we have t… │\n",
       "│ ham    ┆ Will �_ b going to esplanade f… │\n",
       "│ ham    ┆ Pity, * was in mood for that. … │\n",
       "│ ham    ┆ The guy did some bitching but … │\n",
       "│ ham    ┆ Rofl. Its true to its name      │\n",
       "└────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_csv('../../datasets/spam.csv')\n",
    "df = df.drop_nulls()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.rows()\n",
    "corpus = [text for _, text in dataset]\n",
    "y = [label for label,_ in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_clean = [text_pre_processing(text) for text in corpus]\n",
    "tokens_merge = [' '.join(line) for line in tokens_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : 4416\n",
      "Testing Data  : 1105\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tokens_merge, y, stratify=y, test_size=0.2, random_state=42)\n",
    "print(f'Training Data : {len(X_train)}')\n",
    "print(f'Testing Data  : {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data CV : (4416, 1024)\n",
      "Test Data CV : (1105, 1024)\n"
     ]
    }
   ],
   "source": [
    "# train Bag of Words model\n",
    "cv = CountVectorizer(ngram_range = (1, 2), max_features=1024)\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "print(f'Training Data CV : {X_train_cv.shape}')\n",
    "\n",
    "# transform X_test using CV\n",
    "X_test_cv = cv.transform(X_test)\n",
    "print(f'Test Data CV : {X_test_cv.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR  0.97 0.97 0.89\n",
      "KNN 0.96 0.96 0.82\n",
      "NB  0.73 0.77 0.43\n",
      "RFC 0.97 0.97 0.86\n",
      "MLP 0.98 0.98 0.90\n"
     ]
    }
   ],
   "source": [
    "# define the list of classifiers\n",
    "clfs = [\n",
    "    ('LR', LogisticRegression(random_state=42, max_iter=600)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=1)),\n",
    "    ('NB', GaussianNB()),\n",
    "    ('RFC', RandomForestClassifier(random_state=42)),\n",
    "    ('MLP', MLPClassifier(random_state=42, learning_rate='adaptive', max_iter=1000))\n",
    "]\n",
    "\n",
    "# whenever possible used joblib to speed-up the training\n",
    "with joblib.parallel_backend('loky', n_jobs=-1):\n",
    "    for label, clf in clfs:\n",
    "        # train the model\n",
    "        clf.fit(X_train_cv.toarray(), y_train)\n",
    "\n",
    "        # generate predictions\n",
    "        predictions = clf.predict(X_test_cv.toarray())\n",
    "\n",
    "        # compute the performance metrics\n",
    "        mcc = matthews_corrcoef(y_test, predictions)\n",
    "        acc = accuracy_score(y_test, predictions)\n",
    "        f1 = f1_score(y_test, predictions, average='weighted')\n",
    "        print(f'{label:3} {acc:.2f} {f1:.2f} {mcc:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

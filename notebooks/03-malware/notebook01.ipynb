{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing (Polars Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we actually need to import our data. Since our data is stored as a CSV (comma-separated value) file, we will use the `read_csv` function provided by **polars**, which takes our file name as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Country</th><th>Age</th><th>Salary</th><th>Purchased</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;France&quot;</td><td>44</td><td>72000</td><td>&quot;No&quot;</td></tr><tr><td>&quot;Spain&quot;</td><td>27</td><td>48000</td><td>&quot;Yes&quot;</td></tr><tr><td>&quot;Germany&quot;</td><td>30</td><td>54000</td><td>&quot;No&quot;</td></tr><tr><td>&quot;Spain&quot;</td><td>38</td><td>61000</td><td>&quot;No&quot;</td></tr><tr><td>&quot;Germany&quot;</td><td>40</td><td>null</td><td>&quot;Yes&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌─────────┬─────┬────────┬───────────┐\n",
       "│ Country ┆ Age ┆ Salary ┆ Purchased │\n",
       "│ ---     ┆ --- ┆ ---    ┆ ---       │\n",
       "│ str     ┆ i64 ┆ i64    ┆ str       │\n",
       "╞═════════╪═════╪════════╪═══════════╡\n",
       "│ France  ┆ 44  ┆ 72000  ┆ No        │\n",
       "│ Spain   ┆ 27  ┆ 48000  ┆ Yes       │\n",
       "│ Germany ┆ 30  ┆ 54000  ┆ No        │\n",
       "│ Spain   ┆ 38  ┆ 61000  ┆ No        │\n",
       "│ Germany ┆ 40  ┆ null   ┆ Yes       │\n",
       "└─────────┴─────┴────────┴───────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_csv('../../datasets/sample.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>Country</th><th>Age</th><th>Salary</th><th>Purchased</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;10&quot;</td><td>9.0</td><td>9.0</td><td>&quot;10&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>1.0</td><td>1.0</td><td>&quot;0&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>38.777778</td><td>63777.777778</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>7.693793</td><td>12265.579662</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;France&quot;</td><td>27.0</td><td>48000.0</td><td>&quot;No&quot;</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>35.0</td><td>54000.0</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>38.0</td><td>61000.0</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>44.0</td><td>72000.0</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;Spain&quot;</td><td>50.0</td><td>83000.0</td><td>&quot;Yes&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 5)\n",
       "┌────────────┬─────────┬───────────┬──────────────┬───────────┐\n",
       "│ statistic  ┆ Country ┆ Age       ┆ Salary       ┆ Purchased │\n",
       "│ ---        ┆ ---     ┆ ---       ┆ ---          ┆ ---       │\n",
       "│ str        ┆ str     ┆ f64       ┆ f64          ┆ str       │\n",
       "╞════════════╪═════════╪═══════════╪══════════════╪═══════════╡\n",
       "│ count      ┆ 10      ┆ 9.0       ┆ 9.0          ┆ 10        │\n",
       "│ null_count ┆ 0       ┆ 1.0       ┆ 1.0          ┆ 0         │\n",
       "│ mean       ┆ null    ┆ 38.777778 ┆ 63777.777778 ┆ null      │\n",
       "│ std        ┆ null    ┆ 7.693793  ┆ 12265.579662 ┆ null      │\n",
       "│ min        ┆ France  ┆ 27.0      ┆ 48000.0      ┆ No        │\n",
       "│ 25%        ┆ null    ┆ 35.0      ┆ 54000.0      ┆ null      │\n",
       "│ 50%        ┆ null    ┆ 38.0      ┆ 61000.0      ┆ null      │\n",
       "│ 75%        ┆ null    ┆ 44.0      ┆ 72000.0      ┆ null      │\n",
       "│ max        ┆ Spain   ┆ 50.0      ┆ 83000.0      ┆ Yes       │\n",
       "└────────────┴─────────┴───────────┴──────────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "\n",
    "Now we must take care of missing data. Many modern datasets will lack some data and it is very important to deal with it in an intelligent way. One intuition might be to remove a particular observation if it has any missing data. This is generally not considered a great practice because as datasets get larger, we expect many observations to have some sort of missing data.\n",
    "\n",
    "One logical solution, and the one used here, will be to replace any missing values with the **mean of the column**. This allows us to get all of the rest of the information from a given row without skewing the data all that much. \n",
    "\n",
    "We can do this easily in `polars` by selecting the columns we want to impute ('Age' and 'Salary') and using the `fill_null` method, setting the fill value to the `mean` of that column. We use `with_columns` to apply this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10, 4)\n",
      "┌─────────┬───────────┬──────────────┬───────────┐\n",
      "│ Country ┆ Age       ┆ Salary       ┆ Purchased │\n",
      "│ ---     ┆ ---       ┆ ---          ┆ ---       │\n",
      "│ str     ┆ f64       ┆ f64          ┆ str       │\n",
      "╞═════════╪═══════════╪══════════════╪═══════════╡\n",
      "│ France  ┆ 44.0      ┆ 72000.0      ┆ No        │\n",
      "│ Spain   ┆ 27.0      ┆ 48000.0      ┆ Yes       │\n",
      "│ Germany ┆ 30.0      ┆ 54000.0      ┆ No        │\n",
      "│ Spain   ┆ 38.0      ┆ 61000.0      ┆ No        │\n",
      "│ Germany ┆ 40.0      ┆ 63777.777778 ┆ Yes       │\n",
      "│ France  ┆ 35.0      ┆ 58000.0      ┆ Yes       │\n",
      "│ Spain   ┆ 38.777778 ┆ 52000.0      ┆ No        │\n",
      "│ France  ┆ 48.0      ┆ 79000.0      ┆ Yes       │\n",
      "│ Germany ┆ 50.0      ┆ 83000.0      ┆ No        │\n",
      "│ France  ┆ 37.0      ┆ 67000.0      ┆ Yes       │\n",
      "└─────────┴───────────┴──────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Fill missing numeric values with the mean of their respective columns\n",
    "df = df.with_columns([\n",
    "    pl.col('Age').fill_null(pl.col('Age').mean()),\n",
    "    pl.col('Salary').fill_null(pl.col('Salary').mean())\n",
    "])\n",
    "\n",
    "# Display the full data to see the filled values (10 rows in original data)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Variables\n",
    "\n",
    "Now we must encode any categorical variables we might have. When building machine learning models, we are training a mathematical model on data, and strings (like 'Country' and 'Purchased' in this data set) have no meaning as mathematical objects. That is, until we encode them.\n",
    "\n",
    "**1. Feature Variables (X):** For the 'Country' column, we will use one-hot encoding. `polars` provides a simple method `to_dummies` which will create new binary (0 or 1) columns for each country ('Country_France', 'Country_Spain', etc.).\n",
    "\n",
    "**2. Target Variable (y):** For the 'Purchased' column, we need to map 'Yes' and 'No' to numeric values. We will use a `when-then-otherwise` expression to map 'Yes' to 1 and 'No' to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10, 6)\n",
      "┌────────────────┬─────────────────┬───────────────┬───────────┬──────────────┬───────────┐\n",
      "│ Country_France ┆ Country_Germany ┆ Country_Spain ┆ Age       ┆ Salary       ┆ Purchased │\n",
      "│ ---            ┆ ---             ┆ ---           ┆ ---       ┆ ---          ┆ ---       │\n",
      "│ u8             ┆ u8              ┆ u8            ┆ f64       ┆ f64          ┆ i32       │\n",
      "╞════════════════╪═════════════════╪═══════════════╪═══════════╪══════════════╪═══════════╡\n",
      "│ 1              ┆ 0               ┆ 0             ┆ 44.0      ┆ 72000.0      ┆ 0         │\n",
      "│ 0              ┆ 0               ┆ 1             ┆ 27.0      ┆ 48000.0      ┆ 1         │\n",
      "│ 0              ┆ 1               ┆ 0             ┆ 30.0      ┆ 54000.0      ┆ 0         │\n",
      "│ 0              ┆ 0               ┆ 1             ┆ 38.0      ┆ 61000.0      ┆ 0         │\n",
      "│ 0              ┆ 1               ┆ 0             ┆ 40.0      ┆ 63777.777778 ┆ 1         │\n",
      "│ 1              ┆ 0               ┆ 0             ┆ 35.0      ┆ 58000.0      ┆ 1         │\n",
      "│ 0              ┆ 0               ┆ 1             ┆ 38.777778 ┆ 52000.0      ┆ 0         │\n",
      "│ 1              ┆ 0               ┆ 0             ┆ 48.0      ┆ 79000.0      ┆ 1         │\n",
      "│ 0              ┆ 1               ┆ 0             ┆ 50.0      ┆ 83000.0      ┆ 0         │\n",
      "│ 1              ┆ 0               ┆ 0             ┆ 37.0      ┆ 67000.0      ┆ 1         │\n",
      "└────────────────┴─────────────────┴───────────────┴───────────┴──────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical feature 'Country' using one-hot encoding\n",
    "df = df.to_dummies(columns=['Country'])\n",
    "\n",
    "# Encode the target variable 'Purchased'\n",
    "# Map 'Yes' to 1 and 'No' to 0\n",
    "df = df.with_columns(\n",
    "    pl.when(pl.col('Purchased') == 'Yes')\n",
    "      .then(1)\n",
    "      .otherwise(0)\n",
    "      .alias('Purchased') # Overwrite the original column\n",
    "      .cast(pl.Int32)     # Cast to integer\n",
    ")\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Features (X) and Target (y)\n",
    "\n",
    "Now that all our data is numeric and preprocessed, we can separate our features (X) from our target variable (y).\n",
    "\n",
    "* **X (features):** All columns *except* 'Purchased'.\n",
    "* **y (target):** The 'Purchased' column.\n",
    "\n",
    "We will convert these to **NumPy arrays**, as this is the format `sklearn`'s `train_test_split` function expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [[1.00000000e+00 0.00000000e+00 0.00000000e+00 4.40000000e+01\n",
      "  7.20000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 2.70000000e+01\n",
      "  4.80000000e+04]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 3.00000000e+01\n",
      "  5.40000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.80000000e+01\n",
      "  6.10000000e+04]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 4.00000000e+01\n",
      "  6.37777778e+04]]\n",
      "y =  [0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Select all columns *except* 'Purchased' for X\n",
    "X = df.select(pl.all().exclude('Purchased')).to_numpy()\n",
    "\n",
    "# Select only the 'Purchased' column for y\n",
    "y = df.select('Purchased').to_numpy().ravel() # .ravel() for 1D array\n",
    "\n",
    "print(\"X = \", X[:5]) # Print first 5 rows\n",
    "print(\"y = \", y[:5]) # Print first 5 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into Training and Test Sets\n",
    "\n",
    "The penultimate step in data preprocessing it to split our data into a training and a testing sets. What this means is that we want to split up our data into two disjoint sets of observations. The 'train' set of this data is what the model is going to learn on. Once the model has learned on the train data, we will use the 'test' set to see how well the model is able to preform. \n",
    "\n",
    "It is absolutely critical that there is no overlap between these sets. To understand why, consider this scenario. A teacher always gives out a practice exam before the actual exam and wants to know how performance on the practice exam relates to performance on the real exam (the practice exam is our 'train' set and the real exam is our 'test' set). If there is overlap between the questions on the practice exam and the real exam, then the performance on the real exam is going to be biased. \n",
    "\n",
    "The general starting point is 80-20 train-test. `sklearn` provides an extremely easy way to split up into train test, shown below. We will use a 70-30 split (test_size = 0.3) to match the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train =  [[0.00000000e+00 0.00000000e+00 1.00000000e+00 2.70000000e+01\n",
      "  4.80000000e+04]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 4.00000000e+01\n",
      "  6.37777778e+04]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 3.70000000e+01\n",
      "  6.70000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.80000000e+01\n",
      "  6.10000000e+04]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 3.00000000e+01\n",
      "  5.40000000e+04]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 4.80000000e+01\n",
      "  7.90000000e+04]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 5.00000000e+01\n",
      "  8.30000000e+04]] \n",
      "\n",
      "X test =  [[1.00000000e+00 0.00000000e+00 0.00000000e+00 4.40000000e+01\n",
      "  7.20000000e+04]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.87777778e+01\n",
      "  5.20000000e+04]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 3.50000000e+01\n",
      "  5.80000000e+04]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0, stratify=y)\n",
    "print(\"X train = \", X_train, \"\\n\")\n",
    "print(\"X test = \", X_test, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "A final step for many machine learning models is called feature scaling. The intuition behind feature scaling can get a bit mathematical so not much time will be spent trying to understand it, but it is good practice. The idea behind feature scaling is that if variables are on extremely different scales, then the model may become biased. \n",
    "\n",
    "Take for example our Age and Salary features. Age ranges from 27 to 50 while salary 48000 to 83000. If we were to create a model out of this, the term for Salary would dominate the term for Age because the values for Salary are just so much larger and have so much larger of a scale than Age. The remedy is to scale all values to have roughly the same range. \n",
    "\n",
    "We will use `StandardScaler` from `sklearn`. This scales the data to have a mean of 0 and a standard deviation of 1. \n",
    "\n",
    "**Important:** We `fit_transform` on the **training data** (to learn the mean and std) but only `transform` the **test data** (to apply the *same* scaling as the training data). This prevents data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train =  [[-0.63245553 -0.8660254   1.58113883 -1.47345809 -1.46772255]\n",
      " [-0.63245553  1.15470054 -0.63245553  0.18190841 -0.11436799]\n",
      " [ 1.58113883 -0.8660254  -0.63245553 -0.20009925  0.16202132]\n",
      " [-0.63245553 -0.8660254   1.58113883 -0.07276336 -0.35263464]\n",
      " [-0.63245553  1.15470054 -0.63245553 -1.09145044 -0.95306659]\n",
      " [ 1.58113883 -0.8660254  -0.63245553  1.20059548  1.19133324]\n",
      " [-0.63245553  1.15470054 -0.63245553  1.45526725  1.53443721]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "print(\"X train = \", X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now seen a general overview of how to preprocess data for machine learning models. It is critical to undergo these steps as needed before starting any sort of machine learning model to ensure the accuracy and integrity of the model created."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
